{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11049177,"sourceType":"datasetVersion","datasetId":6875854}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:19.040018Z","iopub.execute_input":"2025-03-16T13:50:19.040441Z","iopub.status.idle":"2025-03-16T13:50:19.046240Z","shell.execute_reply.started":"2025-03-16T13:50:19.040416Z","shell.execute_reply":"2025-03-16T13:50:19.045281Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mosaic-train/train_set_f.xlsx\n/kaggle/input/mosaic-train/test_set_f.xlsx\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:20.394787Z","iopub.execute_input":"2025-03-16T13:50:20.395071Z","iopub.status.idle":"2025-03-16T13:50:22.532437Z","shell.execute_reply.started":"2025-03-16T13:50:20.395047Z","shell.execute_reply":"2025-03-16T13:50:22.531725Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Loading dataset from Excel\ntrain_path = \"/kaggle/input/mosaic-train/train_set_f.xlsx\"\ndf = pd.read_excel(train_path)\nlines = df[\"SENTENCES\"].dropna().tolist()\nrandom.shuffle(lines)\n\n# Split into train, validation, and test sets\ntrain_lines, val_lines = train_test_split(lines, test_size=0.1, shuffle=True)\nval_lines, test_lines = train_test_split(val_lines, test_size=0.2, shuffle=True)\n\nprint(f\"Total: {len(lines)}, Train: {len(train_lines)}, Val: {len(val_lines)}, Test: {len(test_lines)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:22.533495Z","iopub.execute_input":"2025-03-16T13:50:22.533910Z","iopub.status.idle":"2025-03-16T13:50:25.411994Z","shell.execute_reply.started":"2025-03-16T13:50:22.533887Z","shell.execute_reply":"2025-03-16T13:50:25.411046Z"}},"outputs":[{"name":"stdout","text":"Total: 50000, Train: 45000, Val: 4000, Test: 1000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.models import WordPiece\nfrom tokenizers import normalizers\nfrom tokenizers.normalizers import NFD, Lowercase, StripAccents\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import WordPieceTrainer\nfrom tokenizers import decoders","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\nbert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\nbert_tokenizer.pre_tokenizer = Whitespace()\nbert_tokenizer.decoder = decoders.WordPiece()\n\n# here we are training the tokenizer on the dataset\ntrainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[MASK]\"], vocab_size=8192)\nbert_tokenizer.train_from_iterator(lines, trainer)\n\n# enabling padding & truncation\nbert_tokenizer.enable_padding(pad_id=bert_tokenizer.token_to_id(\"[PAD]\"), length=128)\nbert_tokenizer.enable_truncation(128)\n\n# we have saved the tokenizer\ntokenizer_path = Path(\"mlm-baby-bert/tokenizer\")\ntokenizer_path.mkdir(exist_ok=True, parents=True)\nbert_tokenizer.save(str(tokenizer_path / \"custom_tokenizer.json\"))\n\nprint(\"Tokenizer trained and saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:26.876135Z","iopub.execute_input":"2025-03-16T13:50:26.876618Z","iopub.status.idle":"2025-03-16T13:50:28.049974Z","shell.execute_reply.started":"2025-03-16T13:50:26.876587Z","shell.execute_reply":"2025-03-16T13:50:28.049142Z"}},"outputs":[{"name":"stdout","text":"Tokenizer trained and saved successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom pathlib import Path\n\n# Loading the trained tokenizer from our saved path\ntokenizer_path = Path(\"mlm-baby-bert/tokenizer/custom_tokenizer.json\")\ntokenizer = Tokenizer.from_file(str(tokenizer_path))\n\n# testing by encoding a sample sentence\nsample_text = \"Hello our team name is Bizli\"\nencoded = tokenizer.encode(sample_text)\n\n# Print tokenized output\nfor token_id, token in zip(encoded.ids, encoded.tokens):\n    print(f\"{token_id}:{token} \", end=\" \")\n\n# Decoding back to text\ndecoded_text = tokenizer.decode(encoded.ids)\nprint(\"\\nDecoded text:\", decoded_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:24:05.509914Z","iopub.execute_input":"2025-03-16T15:24:05.510210Z","iopub.status.idle":"2025-03-16T15:24:05.543170Z","shell.execute_reply.started":"2025-03-16T15:24:05.510186Z","shell.execute_reply":"2025-03-16T15:24:05.542497Z"}},"outputs":[{"name":"stdout","text":"3936:hell  61:##o  989:our  1213:team  654:name  136:is  2208:bi  83:##z  62:##l  70:##i  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  \nDecoded text: hello our team name is bizli\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"class MLMDataset:\n    def __init__(self,lines):\n        self.lines = lines\n    def __len__(self,):\n        return len(self.lines)\n    def __getitem__(self,idx):\n        line = self.lines[idx]\n        ids = tokenizer.encode(line).ids\n        labels = ids.copy()\n        return ids, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:34.489052Z","iopub.execute_input":"2025-03-16T13:50:34.489361Z","iopub.status.idle":"2025-03-16T13:50:34.494007Z","shell.execute_reply.started":"2025-03-16T13:50:34.489336Z","shell.execute_reply":"2025-03-16T13:50:34.493118Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = [torch.tensor(i[0]) for i in batch]\n    labels = [torch.tensor(i[1]) for i in batch]\n    input_ids = torch.stack(input_ids)\n    labels = torch.stack(labels)\n    # mask 15% of text leaving [PAD]\n    mlm_mask = torch.rand(input_ids.size()) < 0.15 * (input_ids!=1)\n    masked_tokens = input_ids * mlm_mask\n    labels[masked_tokens==0]=-100 # seting all tokens except masked tokens to -100\n    input_ids[masked_tokens!=0]=2 # MASK TOKEN\n    return input_ids, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:35.247701Z","iopub.execute_input":"2025-03-16T13:50:35.247958Z","iopub.status.idle":"2025-03-16T13:50:35.252896Z","shell.execute_reply.started":"2025-03-16T13:50:35.247936Z","shell.execute_reply":"2025-03-16T13:50:35.251893Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"ds = MLMDataset(lines)\ndl = torch.utils.data.DataLoader(ds,batch_size=2,shuffle=True,collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:37.127699Z","iopub.execute_input":"2025-03-16T13:50:37.127986Z","iopub.status.idle":"2025-03-16T13:50:37.132354Z","shell.execute_reply.started":"2025-03-16T13:50:37.127964Z","shell.execute_reply":"2025-03-16T13:50:37.131319Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"i,l = next(iter(dl))\nprint(i[1])\nprint(l[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:37.654916Z","iopub.execute_input":"2025-03-16T13:50:37.655172Z","iopub.status.idle":"2025-03-16T13:50:37.668269Z","shell.execute_reply.started":"2025-03-16T13:50:37.655150Z","shell.execute_reply":"2025-03-16T13:50:37.667310Z"}},"outputs":[{"name":"stdout","text":"tensor([   2,    2,  303, 3529,   74,  127,  718,  787,  606,  111, 1379,  265,\n         127,  718,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1])\ntensor([ 300,  282, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\nclass RMSNorm(nn.Module):\n    def __init__(self, d, p=-1., eps=1e-8, bias=False):\n        \"\"\"\n            Root Mean Square Layer Normalization\n        :param d: model size\n        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)\n        :param eps:  epsilon value, default 1e-8\n        :param bias: whether use bias term for RMSNorm, disabled by\n            default because RMSNorm doesn't enforce re-centering invariance.\n        \"\"\"\n        super(RMSNorm, self).__init__()\n\n        self.eps = eps\n        self.d = d\n        self.p = p\n        self.bias = bias\n\n        self.scale = nn.Parameter(torch.ones(d))\n        self.register_parameter(\"scale\", self.scale)\n\n        if self.bias:\n            self.offset = nn.Parameter(torch.zeros(d))\n            self.register_parameter(\"offset\", self.offset)\n\n    def forward(self, x):\n        if self.p < 0. or self.p > 1.:\n            norm_x = x.norm(2, dim=-1, keepdim=True)\n            d_x = self.d\n        else:\n            partial_size = int(self.d * self.p)\n            partial_x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-1)\n\n            norm_x = partial_x.norm(2, dim=-1, keepdim=True)\n            d_x = partial_size\n\n        rms_x = norm_x * d_x ** (-1. / 2)\n        x_normed = x / (rms_x + self.eps)\n\n        if self.bias:\n            return self.scale * x_normed + self.offset\n\n        return self.scale * x_normed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:43.375590Z","iopub.execute_input":"2025-03-16T13:50:43.375892Z","iopub.status.idle":"2025-03-16T13:50:43.382793Z","shell.execute_reply.started":"2025-03-16T13:50:43.375869Z","shell.execute_reply":"2025-03-16T13:50:43.381822Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class MultiheadAttention(nn.Module):\n    def __init__(self, dim, n_heads, dropout=0.):\n        super().__init__()\n        self.dim = dim\n        self.n_heads = n_heads\n        assert dim % n_heads == 0, 'dim should be div by n_heads'\n        self.head_dim = self.dim // self.n_heads\n        self.in_proj = nn.Linear(dim,dim*3,bias=False)\n        self.attn_dropout = nn.Dropout(dropout)\n        self.scale = self.head_dim ** -0.5\n        self.out_proj = nn.Linear(dim,dim)\n\n    def forward(self,x,mask=None):\n        b,t,c = x.shape\n        q,k,v = self.in_proj(x).chunk(3,dim=-1)\n        q = q.view(b,t,self.n_heads,self.head_dim).permute(0,2,1,3)\n        k = k.view(b,t,self.n_heads,self.head_dim).permute(0,2,1,3)\n        v = v.view(b,t,self.n_heads,self.head_dim).permute(0,2,1,3)\n\n        qkT = torch.matmul(q,k.transpose(-1,-2)) * self.scale\n        qkT = self.attn_dropout(qkT)\n\n        if mask is not None:\n            mask = mask.to(dtype=qkT.dtype,device=qkT.device)\n            qkT = qkT.masked_fill(mask==0,float('-inf'))\n\n        qkT = F.softmax(qkT,dim=-1)\n        attn = torch.matmul(qkT,v)\n        attn = attn.permute(0,2,1,3).contiguous().view(b,t,c)\n        out = self.out_proj(attn)\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:45.997145Z","iopub.execute_input":"2025-03-16T13:50:45.997443Z","iopub.status.idle":"2025-03-16T13:50:46.005391Z","shell.execute_reply.started":"2025-03-16T13:50:45.997419Z","shell.execute_reply":"2025-03-16T13:50:46.004424Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self,dim,dropout=0.):\n        super().__init__()\n        self.feed_forward = nn.Sequential(\n            nn.Linear(dim,dim*4),\n            nn.Dropout(dropout),\n            nn.GELU(),\n            nn.Linear(dim*4,dim)\n        )\n\n    def forward(self, x):\n        return self.feed_forward(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:47.742804Z","iopub.execute_input":"2025-03-16T13:50:47.743114Z","iopub.status.idle":"2025-03-16T13:50:47.747667Z","shell.execute_reply.started":"2025-03-16T13:50:47.743085Z","shell.execute_reply":"2025-03-16T13:50:47.746816Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, dim, n_heads, attn_dropout=0., mlp_dropout=0.):\n        super().__init__()\n        self.attn = MultiheadAttention(dim,n_heads,attn_dropout)\n        self.ffd = FeedForward(dim,mlp_dropout)\n        self.ln_1 = RMSNorm(dim)\n        self.ln_2 = RMSNorm(dim)\n\n    def forward(self,x,mask=None):\n        x = self.ln_1(x)\n        x = x + self.attn(x,mask)\n        x = self.ln_2(x)\n        x = x + self.ffd(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:49.268539Z","iopub.execute_input":"2025-03-16T13:50:49.268839Z","iopub.status.idle":"2025-03-16T13:50:49.273886Z","shell.execute_reply.started":"2025-03-16T13:50:49.268815Z","shell.execute_reply":"2025-03-16T13:50:49.272942Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Embedding(nn.Module):\n    def __init__(self,vocab_size,max_len,dim):\n        super().__init__()\n        self.max_len = max_len\n        self.class_embedding = nn.Embedding(vocab_size,dim)\n        self.pos_embedding = nn.Embedding(max_len,dim)\n    def forward(self,x):\n        x = self.class_embedding(x)\n        pos = torch.arange(0,x.size(1),device=x.device)\n        x = x + self.pos_embedding(pos)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:51.019800Z","iopub.execute_input":"2025-03-16T13:50:51.020120Z","iopub.status.idle":"2025-03-16T13:50:51.024780Z","shell.execute_reply.started":"2025-03-16T13:50:51.020091Z","shell.execute_reply":"2025-03-16T13:50:51.023926Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class MLMBERT(nn.Module):\n    def __init__(self, config):\n\n        super().__init__()\n\n        self.embedding = Embedding(config['vocab_size'],config['max_len'],config['dim'])\n\n        self.depth = config['depth']\n        self.encoders = nn.ModuleList([\n            EncoderBlock(\n                dim=config['dim'],\n                n_heads=config['n_heads'],\n                attn_dropout=config['attn_dropout'],\n                mlp_dropout=config['mlp_dropout']\n            ) for _ in range(self.depth)\n        ])\n\n        self.ln_f = RMSNorm(config['dim'])\n\n        self.mlm_head = nn.Linear(config['dim'],config['vocab_size'],bias=False)\n\n        self.embedding.class_embedding.weight = self.mlm_head.weight #  tying the weights\n\n        self.pad_token_id = config['pad_token_id']\n        self.mask_token_id = config['mask_token_id']\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def create_src_mask(self,src):\n        return (src != self.pad_token_id).unsqueeze(1).unsqueeze(2) \n\n    def forward(self,input_ids,labels=None):\n\n        src_mask = self.create_src_mask(input_ids)\n        enc_out = self.embedding(input_ids)\n        for layer in self.encoders:\n            enc_out = layer(enc_out,mask=src_mask)\n\n        enc_out = self.ln_f(enc_out)\n\n        logits = self.mlm_head(enc_out)\n\n        if labels is not None:\n            loss = F.cross_entropy(logits.view(-1,logits.size(-1)),labels.view(-1))\n            return {'loss': loss, 'logits': logits}\n        else:\n            # inference should have only one masked token\n            mask_idx = (input_ids==self.mask_token_id).flatten().nonzero().item()\n            mask_preds = F.softmax(logits[:,mask_idx,:],dim=-1).argmax(dim=-1)\n            return {'mask_predictions':mask_preds}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:52.585947Z","iopub.execute_input":"2025-03-16T13:50:52.586387Z","iopub.status.idle":"2025-03-16T13:50:52.602883Z","shell.execute_reply.started":"2025-03-16T13:50:52.586348Z","shell.execute_reply":"2025-03-16T13:50:52.601924Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#setting the parameters of the model\nconfig = {\n    'dim': 256,\n    'n_heads': 8,\n    'attn_dropout': 0.1,\n    'mlp_dropout': 0.1,\n    'depth': 6,\n    'vocab_size': 8192,\n    'max_len': 128,\n    'pad_token_id': 1,\n    'mask_token_id': 2\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:55.647379Z","iopub.execute_input":"2025-03-16T13:50:55.647733Z","iopub.status.idle":"2025-03-16T13:50:55.651688Z","shell.execute_reply.started":"2025-03-16T13:50:55.647706Z","shell.execute_reply":"2025-03-16T13:50:55.650865Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MLMBERT(config).to(device)\nprint(f'Model is running on: {device}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:56.665052Z","iopub.execute_input":"2025-03-16T13:50:56.665347Z","iopub.status.idle":"2025-03-16T13:50:57.001123Z","shell.execute_reply.started":"2025-03-16T13:50:56.665321Z","shell.execute_reply":"2025-03-16T13:50:57.000329Z"}},"outputs":[{"name":"stdout","text":"Model is running on: cuda\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print('trainable:',sum([p.numel() for p in model.parameters() if p.requires_grad]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:58.820650Z","iopub.execute_input":"2025-03-16T13:50:58.820931Z","iopub.status.idle":"2025-03-16T13:50:58.826644Z","shell.execute_reply.started":"2025-03-16T13:50:58.820910Z","shell.execute_reply":"2025-03-16T13:50:58.825352Z"}},"outputs":[{"name":"stdout","text":"trainable: 6861056\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"train_ds = MLMDataset(train_lines)\nval_ds = MLMDataset(val_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:50:59.968886Z","iopub.execute_input":"2025-03-16T13:50:59.969184Z","iopub.status.idle":"2025-03-16T13:50:59.972683Z","shell.execute_reply.started":"2025-03-16T13:50:59.969161Z","shell.execute_reply":"2025-03-16T13:50:59.971881Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:51:02.066116Z","iopub.execute_input":"2025-03-16T13:51:02.066408Z","iopub.status.idle":"2025-03-16T13:51:02.070723Z","shell.execute_reply.started":"2025-03-16T13:51:02.066385Z","shell.execute_reply":"2025-03-16T13:51:02.069795Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# TEST : SINGLE TOKEN MASKING\n\ntest_actuals = []\ntest_batches = []\nfor ln in tqdm(test_lines):\n    tokenized = tokenizer.encode(ln)\n    fi = len(tokenized.ids)\n    if 1 in tokenized.special_tokens_mask:\n        fi = torch.tensor(tokenized.special_tokens_mask).nonzero()[0].item() # ignore [PAD]\n    m = torch.randint(0,fi,(1,)).item() # select random token to mask\n    input_ids = torch.tensor(tokenized.ids)\n    test_actuals.append(input_ids[m].item())\n    input_ids[m]=2 # replace with [MASK]\n    test_batches.append(input_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:51:03.110504Z","iopub.execute_input":"2025-03-16T13:51:03.110798Z","iopub.status.idle":"2025-03-16T13:51:03.274383Z","shell.execute_reply.started":"2025-03-16T13:51:03.110774Z","shell.execute_reply":"2025-03-16T13:51:03.273511Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"149d3039a37c4f2a924861a7e7892cc7"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"epochs = 60\ntrain_losses = []\nvalid_losses = []\ntest_accuracies = []\nbest_val_loss = 1e9","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:51:05.949349Z","iopub.execute_input":"2025-03-16T13:51:05.949666Z","iopub.status.idle":"2025-03-16T13:51:05.953395Z","shell.execute_reply.started":"2025-03-16T13:51:05.949643Z","shell.execute_reply":"2025-03-16T13:51:05.952672Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from torch.optim import AdamW\noptim = AdamW(model.parameters(),lr=6e-4 / 25., weight_decay=0.01)\nsched = torch.optim.lr_scheduler.OneCycleLR(optim,max_lr=6e-4,steps_per_epoch=len(train_dl),epochs=epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:51:07.338688Z","iopub.execute_input":"2025-03-16T13:51:07.338980Z","iopub.status.idle":"2025-03-16T13:51:08.217504Z","shell.execute_reply.started":"2025-03-16T13:51:07.338957Z","shell.execute_reply":"2025-03-16T13:51:08.216815Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import os\nimport torch\n\n# Path to save checkpoint\ncheckpoint_path = './mlm-baby-bert/checkpoint.pth'\n\n# Checking if a checkpoint exists and resuming from the last epoch\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    start_epoch = checkpoint['epoch'] + 1  # Resume from next epoch\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optim.load_state_dict(checkpoint['optimizer_state_dict'])\n    sched.load_state_dict(checkpoint['scheduler_state_dict'])\n    train_losses = checkpoint['train_losses']\n    valid_losses = checkpoint['valid_losses']\n    test_accuracies = checkpoint['test_accuracies']\n    best_val_loss = checkpoint['best_val_loss']\n    print(f\"Resuming training from epoch {start_epoch}\")\nelse:\n    start_epoch = 0  # Starting from scratch if no previous checkpoint is found\n    print(\"No checkpoint found. Training from scratch.\")\n\nfor ep in tqdm(range(start_epoch, epochs)):  # Starting from saved epoch\n    model.train()\n    trl = 0.\n    tprog = tqdm(enumerate(train_dl), total=len(train_dl))\n    for i, (input_ids, labels) in tprog:\n        input_ids = input_ids.to('cuda')\n        labels = labels.to('cuda')\n        loss = model(input_ids, labels)['loss']\n        loss.backward()\n        optim.step()\n        optim.zero_grad()\n        sched.step()\n        trl += loss.item()\n        tprog.set_description(f'train step loss: {loss.item():.4f}')\n    train_losses.append(trl / len(train_dl))\n\n    model.eval()\n    with torch.no_grad():\n        vrl = 0.\n        vprog = tqdm(enumerate(val_dl), total=len(val_dl))\n        for i, (input_ids, labels) in vprog:\n            input_ids = input_ids.to('cuda')\n            labels = labels.to('cuda')\n            loss = model(input_ids, labels)['loss']\n            vrl += loss.item()\n            vprog.set_description(f'valid step loss: {loss.item():.4f}')\n        vloss = vrl / len(val_dl)\n        valid_losses.append(vloss)\n        print(f'epoch {ep} | train_loss: {train_losses[-1]:.4f} valid_loss: {valid_losses[-1]:.4f}')\n\n        if vloss < best_val_loss:\n            best_val_loss = vloss\n            print('PREDICTING!')\n            test_predictions = []\n            for input_ids in tqdm(test_batches):\n                input_ids = input_ids.unsqueeze(0)\n                input_ids = input_ids.to('cuda')\n                mask_preds = model(input_ids)['mask_predictions']\n                test_predictions.extend(list(mask_preds.detach().cpu().flatten().numpy()))\n\n            tacc = accuracy_score(test_actuals, test_predictions)\n            test_accuracies.append(tacc)\n            print(f'SINGLE MASK TOKEN PREDICTION ACCURACY: {tacc:.4f}')\n            print('saving best model...')\n            torch.save(model.state_dict(), './mlm-baby-bert/model.pt')\n\n    # *Saving training progress (checkpoint)*\n    checkpoint = {\n        'epoch': ep,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optim.state_dict(),\n        'scheduler_state_dict': sched.state_dict(),\n        'train_losses': train_losses,\n        'valid_losses': valid_losses,\n        'test_accuracies': test_accuracies,\n        'best_val_loss': best_val_loss\n    }\n    torch.save(checkpoint, checkpoint_path)  # Saving after every epoch\n\n    if test_accuracies:\n        print(f'epoch {ep} | accuracy: {test_accuracies[-1]:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:51:08.357121Z","iopub.execute_input":"2025-03-16T13:51:08.357507Z","iopub.status.idle":"2025-03-16T14:20:46.531685Z","shell.execute_reply.started":"2025-03-16T13:51:08.357481Z","shell.execute_reply":"2025-03-16T14:20:46.530670Z"}},"outputs":[{"name":"stdout","text":"Resuming training from epoch 45\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-26-5010407c0c14>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"694d7ce3f1314cc4a0d92ce3314e9f69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9ba56c558c4d5db49ae97d4ca2ffa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"394f4046546f4e18990373d65009a0b9"}},"metadata":{}},{"name":"stdout","text":"epoch 45 | train_loss: 3.5055 valid_loss: 3.3448\nPREDICTING!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fc184a526cf4529bebf48bbb77d61fa"}},"metadata":{}},{"name":"stdout","text":"SINGLE MASK TOKEN PREDICTION ACCURACY: 0.4710\nsaving best model...\nepoch 45 | accuracy: 0.4710\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aba901b6b4f04554b5677947e73ccfed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d9fa483cb694428985cc24115e7afce"}},"metadata":{}},{"name":"stdout","text":"epoch 46 | train_loss: 3.3120 valid_loss: 3.2532\nPREDICTING!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5faa5d6be0784d55940d23a997b8aed7"}},"metadata":{}},{"name":"stdout","text":"SINGLE MASK TOKEN PREDICTION ACCURACY: 0.4710\nsaving best model...\nepoch 46 | accuracy: 0.4710\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f5cace6cea341bf8ce52ba2a97c47f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed7548bed3c14b37b81640103266d1ee"}},"metadata":{}},{"name":"stdout","text":"epoch 47 | train_loss: 3.2011 valid_loss: 3.3464\nepoch 47 | accuracy: 0.4710\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54446cb1d18248df8555f3a8b08b49a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab463182268340f0a306c94ba744cb1a"}},"metadata":{}},{"name":"stdout","text":"epoch 48 | train_loss: 3.1405 valid_loss: 3.2728\nepoch 48 | accuracy: 0.4710\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82804f9e2df6415c9b5d6845131b335e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f17801e6dcb0463390c7ae059bb11713"}},"metadata":{}},{"name":"stdout","text":"epoch 49 | train_loss: 3.0821 valid_loss: 3.2055\nPREDICTING!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35f76e61b57343b3bb1a8a6006d246e8"}},"metadata":{}},{"name":"stdout","text":"SINGLE MASK TOKEN PREDICTION ACCURACY: 0.4820\nsaving best model...\nepoch 49 | accuracy: 0.4820\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5913771df5ac4d7d898436de67750d9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34151deb174e4b84a113a8469fc6e821"}},"metadata":{}},{"name":"stdout","text":"epoch 50 | train_loss: 3.0477 valid_loss: 3.2619\nepoch 50 | accuracy: 0.4820\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d88fdc8ac2244ebbbcab2879f536856"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"154741f4ae814a1b950fa01fb6a5ca00"}},"metadata":{}},{"name":"stdout","text":"epoch 51 | train_loss: 3.0105 valid_loss: 3.2236\nepoch 51 | accuracy: 0.4820\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e629f8dfbfc349e3bd91c51d2b712422"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a78ead07e6e49feb712d9e139347d67"}},"metadata":{}},{"name":"stdout","text":"epoch 52 | train_loss: 3.0067 valid_loss: 3.2291\nepoch 52 | accuracy: 0.4820\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7902a6abf9814038b3bd00b2cea1b78b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab148b9db3bb473ab2784909eec8ac87"}},"metadata":{}},{"name":"stdout","text":"epoch 53 | train_loss: 2.9549 valid_loss: 3.1898\nPREDICTING!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b003911471143d795f813187c39f867"}},"metadata":{}},{"name":"stdout","text":"SINGLE MASK TOKEN PREDICTION ACCURACY: 0.4840\nsaving best model...\nepoch 53 | accuracy: 0.4840\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"944b3b2b36bd4a41a89f20f75477f5ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a75e675c11f4f568fbc78c5c19067bc"}},"metadata":{}},{"name":"stdout","text":"epoch 54 | train_loss: 2.9380 valid_loss: 3.1653\nPREDICTING!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0c5ea4e483740b2913165d4b011ba0b"}},"metadata":{}},{"name":"stdout","text":"SINGLE MASK TOKEN PREDICTION ACCURACY: 0.4790\nsaving best model...\nepoch 54 | accuracy: 0.4790\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"775ab3d5add640569f3904d978b48913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8fcdcca7a574038922508b4eb719451"}},"metadata":{}},{"name":"stdout","text":"epoch 55 | train_loss: 2.9337 valid_loss: 3.1860\nepoch 55 | accuracy: 0.4790\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b0e2ff886a64f59bb0813f5fc267bdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa627b0b493e4547938f4455609a80be"}},"metadata":{}},{"name":"stdout","text":"epoch 56 | train_loss: 2.9297 valid_loss: 3.1593\nPREDICTING!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"955f0f23678a450abca5069f19a11e94"}},"metadata":{}},{"name":"stdout","text":"SINGLE MASK TOKEN PREDICTION ACCURACY: 0.4900\nsaving best model...\nepoch 56 | accuracy: 0.4900\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"317e501a064c48ec800aea89c5ebaf3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74cdc5b929aa4d1691a3d85ea71b66e9"}},"metadata":{}},{"name":"stdout","text":"epoch 57 | train_loss: 2.8993 valid_loss: 3.2641\nepoch 57 | accuracy: 0.4900\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cc6352bb4614619a3e0c6377cdeb319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f15c7f2df764b4ebb960722ee20b765"}},"metadata":{}},{"name":"stdout","text":"epoch 58 | train_loss: 2.9107 valid_loss: 3.0955\nPREDICTING!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a43fd32a1e4e3d8096ac2206bcd4b4"}},"metadata":{}},{"name":"stdout","text":"SINGLE MASK TOKEN PREDICTION ACCURACY: 0.4890\nsaving best model...\nepoch 58 | accuracy: 0.4890\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/352 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f23a835fca146ff8e60c5195380213e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"951e70de60684f8bbab95eb12d68a1ee"}},"metadata":{}},{"name":"stdout","text":"epoch 59 | train_loss: 2.9174 valid_loss: 3.1610\nepoch 59 | accuracy: 0.4890\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"\nsd = torch.load('./mlm-baby-bert/model.pt')\nmodel.load_state_dict(sd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:10:14.707090Z","iopub.execute_input":"2025-03-16T15:10:14.707395Z","iopub.status.idle":"2025-03-16T15:10:14.745012Z","shell.execute_reply.started":"2025-03-16T15:10:14.707369Z","shell.execute_reply":"2025-03-16T15:10:14.744296Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-39-09746dcb7749>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  sd = torch.load('./mlm-baby-bert/model.pt')\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"def predict_mask(sentence):\n    x = tokenizer.encode(sentence)\n    \n    try:\n        idx = x.ids.index(tokenizer.token_to_id('[MASK]'))  \n    except ValueError:\n        print(\"No [MASK] token found in the sentence!\")\n        return\n    \n    input_ids = x.ids.copy()\n    \n    # Preparing input\n    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to('cuda')\n    \n    # Model prediction\n    out = model(input_tensor)\n    predicted_id = out['mask_predictions'].item()\n    predicted_token = tokenizer.decode([predicted_id])\n    \n    # Replacing [MASK] with the predicted token\n    predicted_sentence = input_ids.copy()\n    predicted_sentence[idx] = predicted_id\n    \n    print(f'ACTUAL:  {sentence}')\n    print(f'PREDICTED: {tokenizer.decode(predicted_sentence, skip_special_tokens=True)}')\n\n    return predicted_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:10:17.728292Z","iopub.execute_input":"2025-03-16T15:10:17.728600Z","iopub.status.idle":"2025-03-16T15:10:17.736023Z","shell.execute_reply.started":"2025-03-16T15:10:17.728566Z","shell.execute_reply":"2025-03-16T15:10:17.734941Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"predict_mask(\"His second [MASK] Chaya Malka was a daughter of Rabbi Yisroel Friedman of Ruzhin .\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T15:10:21.128898Z","iopub.execute_input":"2025-03-16T15:10:21.129193Z","iopub.status.idle":"2025-03-16T15:10:21.144384Z","shell.execute_reply.started":"2025-03-16T15:10:21.129167Z","shell.execute_reply":"2025-03-16T15:10:21.143545Z"}},"outputs":[{"name":"stdout","text":"ACTUAL:  His second [MASK] Chaya Malka was a daughter of Rabbi Yisroel Friedman of Ruzhin .\nPREDICTED: his second wife chaya malka was a daughter of rabbi yisroel friedman of ruzhin.\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'wife'"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"import pandas as pd\nimport torch\n\n# Load Excel file\ndf = pd.read_excel(\"/kaggle/input/mosaic-train/test_set_f.xlsx\")  # Change file path if needed\n\n# List for storing predictions\npredictions = []\n\n# Function to predict masked words\ndef predict_mask(sentence):\n    x = tokenizer.encode(sentence)\n    \n    # Find the index of [MASK] token in the input sentence\n    try:\n        idx = x.ids.index(tokenizer.token_to_id('[MASK]'))  # Get index of [MASK]\n    except ValueError:\n        print(\"No [MASK] token found in the sentence!\")\n        return\n    \n    input_ids = x.ids.copy()\n    \n    # Preparing input\n    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to('cuda')\n    \n    # Model prediction\n    out = model(input_tensor)\n    predicted_id = out['mask_predictions'].item()\n    predicted_token = tokenizer.decode([predicted_id])\n    \n    # Replacing [MASK] with the predicted token\n    predicted_sentence = input_ids.copy()\n    predicted_sentence[idx] = predicted_id\n    \n   # print(f'ACTUAL:  {sentence}')\n    #print(f'PREDICTED: {tokenizer.decode(predicted_sentence, skip_special_tokens=True)}')\n\n    return predicted_token\n\n# Apply prediction to each sentence\nfor sentence in df[\"MASKED SENTENCES\"].fillna(\"NO_SENTENCE\"):  # Handle missing values\n    pred_word = predict_mask(sentence)\n    predictions.append(pred_word)\n\n# Add predictions to DataFrame\ndf[\"PREDICTED_WORD\"] = predictions\n\n# **Updated Output Path**\noutput_path = \"/kaggle/working/predictions.csv\"  # Ensure it's saved in Kaggle working directory\ndf.to_csv(output_path, index=False)\n\nprint(f\"✅ Predictions saved to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T14:27:53.234784Z","iopub.execute_input":"2025-03-16T14:27:53.235111Z","iopub.status.idle":"2025-03-16T14:28:57.787626Z","shell.execute_reply.started":"2025-03-16T14:27:53.235067Z","shell.execute_reply":"2025-03-16T14:28:57.786698Z"}},"outputs":[{"name":"stdout","text":"✅ Predictions saved to /kaggle/working/predictions.csv\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T14:46:45.498106Z","iopub.execute_input":"2025-03-16T14:46:45.498445Z","iopub.status.idle":"2025-03-16T14:46:45.505404Z","shell.execute_reply.started":"2025-03-16T14:46:45.498419Z","shell.execute_reply":"2025-03-16T14:46:45.504564Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T14:47:10.134058Z","iopub.execute_input":"2025-03-16T14:47:10.134344Z","iopub.status.idle":"2025-03-16T14:47:10.147971Z","shell.execute_reply.started":"2025-03-16T14:47:10.134321Z","shell.execute_reply":"2025-03-16T14:47:10.147151Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'INVALID_MASK_INDEX_10'"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}